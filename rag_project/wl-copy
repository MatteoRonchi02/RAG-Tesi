import os
from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader
from langchain_huggingface import HuggingFaceEmbeddings
#from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
import streamlit as st

# Percorso al modello Mistral GGUF
#MODEL_PATH = "/home/teo/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

# Caricamento del modello locale con LlamaCpp
llm = OllamaLLM(model="mistral")

# Caricamento documenti dalla knowledge base
loader = DirectoryLoader(
    'knowledge_base',
    glob='*.md',
    loader_cls=UnstructuredMarkdownLoader
)
documents = loader.load()

# Suddivisione in chunk
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
docs = text_splitter.split_documents(documents)

# Creazione degli embedding 
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)

# Creazione della pipeline RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# Interfaccia Streamlit
st.title("RAG con Mistral per il rilevamento di Security Smell")
query = st.text_input("Inserisci la tua domanda:")
if query:
    response = qa_chain.run(query)
    st.write(response)
